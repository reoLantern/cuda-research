//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36037853
// Cuda compilation tools, release 12.9, V12.9.86
// Based on NVVM 7.0.1
//

.version 8.8
.target sm_80
.address_size 64

	// .globl	s64_lt

.visible .entry s64_lt(
	.param .u64 s64_lt_param_0,
	.param .u64 s64_lt_param_1,
	.param .u64 s64_lt_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;
	.loc	1 50 0


	ld.param.u64 	%rd1, [s64_lt_param_0];
	ld.param.u64 	%rd2, [s64_lt_param_1];
	ld.param.u64 	%rd3, [s64_lt_param_2];
	.loc	1 50 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 8;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u64 	%rd10, [%rd9];
	add.s64 	%rd11, %rd5, %rd8;
	ld.global.nc.u64 	%rd12, [%rd11];
	setp.lt.s64 	%p1, %rd10, %rd12;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd13, %rd4, %rd7;
	st.global.u8 	[%rd13], %rs1;
	ret;

}
	// .globl	s64_le
.visible .entry s64_le(
	.param .u64 s64_le_param_0,
	.param .u64 s64_le_param_1,
	.param .u64 s64_le_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;
	.loc	1 51 0


	ld.param.u64 	%rd1, [s64_le_param_0];
	ld.param.u64 	%rd2, [s64_le_param_1];
	ld.param.u64 	%rd3, [s64_le_param_2];
	.loc	1 51 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 8;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u64 	%rd10, [%rd9];
	add.s64 	%rd11, %rd5, %rd8;
	ld.global.nc.u64 	%rd12, [%rd11];
	setp.le.s64 	%p1, %rd10, %rd12;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd13, %rd4, %rd7;
	st.global.u8 	[%rd13], %rs1;
	ret;

}
	// .globl	s64_gt
.visible .entry s64_gt(
	.param .u64 s64_gt_param_0,
	.param .u64 s64_gt_param_1,
	.param .u64 s64_gt_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;
	.loc	1 52 0


	ld.param.u64 	%rd1, [s64_gt_param_0];
	ld.param.u64 	%rd2, [s64_gt_param_1];
	ld.param.u64 	%rd3, [s64_gt_param_2];
	.loc	1 52 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 8;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u64 	%rd10, [%rd9];
	add.s64 	%rd11, %rd5, %rd8;
	ld.global.nc.u64 	%rd12, [%rd11];
	setp.gt.s64 	%p1, %rd10, %rd12;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd13, %rd4, %rd7;
	st.global.u8 	[%rd13], %rs1;
	ret;

}
	// .globl	s64_ge
.visible .entry s64_ge(
	.param .u64 s64_ge_param_0,
	.param .u64 s64_ge_param_1,
	.param .u64 s64_ge_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;
	.loc	1 53 0


	ld.param.u64 	%rd1, [s64_ge_param_0];
	ld.param.u64 	%rd2, [s64_ge_param_1];
	ld.param.u64 	%rd3, [s64_ge_param_2];
	.loc	1 53 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 8;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u64 	%rd10, [%rd9];
	add.s64 	%rd11, %rd5, %rd8;
	ld.global.nc.u64 	%rd12, [%rd11];
	setp.ge.s64 	%p1, %rd10, %rd12;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd13, %rd4, %rd7;
	st.global.u8 	[%rd13], %rs1;
	ret;

}
	// .globl	s64_eq
.visible .entry s64_eq(
	.param .u64 s64_eq_param_0,
	.param .u64 s64_eq_param_1,
	.param .u64 s64_eq_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;
	.loc	1 54 0


	ld.param.u64 	%rd1, [s64_eq_param_0];
	ld.param.u64 	%rd2, [s64_eq_param_1];
	ld.param.u64 	%rd3, [s64_eq_param_2];
	.loc	1 54 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 8;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u64 	%rd10, [%rd9];
	add.s64 	%rd11, %rd5, %rd8;
	ld.global.nc.u64 	%rd12, [%rd11];
	setp.eq.s64 	%p1, %rd10, %rd12;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd13, %rd4, %rd7;
	st.global.u8 	[%rd13], %rs1;
	ret;

}
	// .globl	s64_ne
.visible .entry s64_ne(
	.param .u64 s64_ne_param_0,
	.param .u64 s64_ne_param_1,
	.param .u64 s64_ne_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;
	.loc	1 55 0


	ld.param.u64 	%rd1, [s64_ne_param_0];
	ld.param.u64 	%rd2, [s64_ne_param_1];
	ld.param.u64 	%rd3, [s64_ne_param_2];
	.loc	1 55 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 8;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u64 	%rd10, [%rd9];
	add.s64 	%rd11, %rd5, %rd8;
	ld.global.nc.u64 	%rd12, [%rd11];
	setp.ne.s64 	%p1, %rd10, %rd12;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd13, %rd4, %rd7;
	st.global.u8 	[%rd13], %rs1;
	ret;

}
	// .globl	u64_lt
.visible .entry u64_lt(
	.param .u64 u64_lt_param_0,
	.param .u64 u64_lt_param_1,
	.param .u64 u64_lt_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;
	.loc	1 58 0


	ld.param.u64 	%rd1, [u64_lt_param_0];
	ld.param.u64 	%rd2, [u64_lt_param_1];
	ld.param.u64 	%rd3, [u64_lt_param_2];
	.loc	1 58 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 8;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u64 	%rd10, [%rd9];
	add.s64 	%rd11, %rd5, %rd8;
	ld.global.nc.u64 	%rd12, [%rd11];
	setp.lt.u64 	%p1, %rd10, %rd12;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd13, %rd4, %rd7;
	st.global.u8 	[%rd13], %rs1;
	ret;

}
	// .globl	u64_le
.visible .entry u64_le(
	.param .u64 u64_le_param_0,
	.param .u64 u64_le_param_1,
	.param .u64 u64_le_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;
	.loc	1 59 0


	ld.param.u64 	%rd1, [u64_le_param_0];
	ld.param.u64 	%rd2, [u64_le_param_1];
	ld.param.u64 	%rd3, [u64_le_param_2];
	.loc	1 59 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 8;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u64 	%rd10, [%rd9];
	add.s64 	%rd11, %rd5, %rd8;
	ld.global.nc.u64 	%rd12, [%rd11];
	setp.le.u64 	%p1, %rd10, %rd12;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd13, %rd4, %rd7;
	st.global.u8 	[%rd13], %rs1;
	ret;

}
	// .globl	u64_gt
.visible .entry u64_gt(
	.param .u64 u64_gt_param_0,
	.param .u64 u64_gt_param_1,
	.param .u64 u64_gt_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;
	.loc	1 60 0


	ld.param.u64 	%rd1, [u64_gt_param_0];
	ld.param.u64 	%rd2, [u64_gt_param_1];
	ld.param.u64 	%rd3, [u64_gt_param_2];
	.loc	1 60 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 8;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u64 	%rd10, [%rd9];
	add.s64 	%rd11, %rd5, %rd8;
	ld.global.nc.u64 	%rd12, [%rd11];
	setp.gt.u64 	%p1, %rd10, %rd12;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd13, %rd4, %rd7;
	st.global.u8 	[%rd13], %rs1;
	ret;

}
	// .globl	u64_ge
.visible .entry u64_ge(
	.param .u64 u64_ge_param_0,
	.param .u64 u64_ge_param_1,
	.param .u64 u64_ge_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;
	.loc	1 61 0


	ld.param.u64 	%rd1, [u64_ge_param_0];
	ld.param.u64 	%rd2, [u64_ge_param_1];
	ld.param.u64 	%rd3, [u64_ge_param_2];
	.loc	1 61 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 8;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u64 	%rd10, [%rd9];
	add.s64 	%rd11, %rd5, %rd8;
	ld.global.nc.u64 	%rd12, [%rd11];
	setp.ge.u64 	%p1, %rd10, %rd12;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd13, %rd4, %rd7;
	st.global.u8 	[%rd13], %rs1;
	ret;

}
	// .globl	u64_eq
.visible .entry u64_eq(
	.param .u64 u64_eq_param_0,
	.param .u64 u64_eq_param_1,
	.param .u64 u64_eq_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;
	.loc	1 62 0


	ld.param.u64 	%rd1, [u64_eq_param_0];
	ld.param.u64 	%rd2, [u64_eq_param_1];
	ld.param.u64 	%rd3, [u64_eq_param_2];
	.loc	1 62 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 8;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u64 	%rd10, [%rd9];
	add.s64 	%rd11, %rd5, %rd8;
	ld.global.nc.u64 	%rd12, [%rd11];
	setp.eq.s64 	%p1, %rd10, %rd12;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd13, %rd4, %rd7;
	st.global.u8 	[%rd13], %rs1;
	ret;

}
	// .globl	u64_ne
.visible .entry u64_ne(
	.param .u64 u64_ne_param_0,
	.param .u64 u64_ne_param_1,
	.param .u64 u64_ne_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<14>;
	.loc	1 63 0


	ld.param.u64 	%rd1, [u64_ne_param_0];
	ld.param.u64 	%rd2, [u64_ne_param_1];
	ld.param.u64 	%rd3, [u64_ne_param_2];
	.loc	1 63 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 8;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u64 	%rd10, [%rd9];
	add.s64 	%rd11, %rd5, %rd8;
	ld.global.nc.u64 	%rd12, [%rd11];
	setp.ne.s64 	%p1, %rd10, %rd12;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd13, %rd4, %rd7;
	st.global.u8 	[%rd13], %rs1;
	ret;

}
	// .globl	s32_lt
.visible .entry s32_lt(
	.param .u64 s32_lt_param_0,
	.param .u64 s32_lt_param_1,
	.param .u64 s32_lt_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<12>;
	.loc	1 66 0


	ld.param.u64 	%rd1, [s32_lt_param_0];
	ld.param.u64 	%rd2, [s32_lt_param_1];
	ld.param.u64 	%rd3, [s32_lt_param_2];
	.loc	1 66 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 4;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u32 	%r5, [%rd9];
	add.s64 	%rd10, %rd5, %rd8;
	ld.global.nc.u32 	%r6, [%rd10];
	setp.lt.s32 	%p1, %r5, %r6;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd11, %rd4, %rd7;
	st.global.u8 	[%rd11], %rs1;
	ret;

}
	// .globl	u32_lt
.visible .entry u32_lt(
	.param .u64 u32_lt_param_0,
	.param .u64 u32_lt_param_1,
	.param .u64 u32_lt_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<12>;
	.loc	1 67 0


	ld.param.u64 	%rd1, [u32_lt_param_0];
	ld.param.u64 	%rd2, [u32_lt_param_1];
	ld.param.u64 	%rd3, [u32_lt_param_2];
	.loc	1 67 41
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.s64.s32 	%rd7, %r4;
	mul.wide.s32 	%rd8, %r4, 4;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.nc.u32 	%r5, [%rd9];
	add.s64 	%rd10, %rd5, %rd8;
	ld.global.nc.u32 	%r6, [%rd10];
	setp.lt.u32 	%p1, %r5, %r6;
	selp.u16 	%rs1, 1, 0, %p1;
	add.s64 	%rd11, %rd4, %rd7;
	st.global.u8 	[%rd11], %rs1;
	ret;

}

	.file	1 "/home/mmy/work/play/cuda-research/cuda-code/23_compare64bit/compare64.cu"
