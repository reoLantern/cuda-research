
Fatbin elf code:
================
arch = sm_75
code version = [1,7]
host = linux
compile_size = 64bit

Fatbin elf code:
================
arch = sm_75
code version = [1,7]
host = linux
compile_size = 64bit

Fatbin ptx code:
================
arch = sm_75
code version = [8,4]
host = linux
compile_size = 64bit
compressed
ptxasOptions = 

//
//
//
//
//
//

.version 8.4
.target sm_75
.address_size 64

//
.extern .func (.param .b32 func_retval0) vprintf
(
.param .b64 vprintf_param_0,
.param .b64 vprintf_param_1
)
;
//
//
//
.global .align 1 .b8 $str[10] = {82, 67, 91, 48, 126, 55, 93, 58, 32};
.global .align 1 .b8 $str$1[4] = {37, 100, 32};
.global .align 1 .b8 $str$2[2] = {10};

.visible .entry _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm(
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_0,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_1,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_2,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_3,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_4,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_5
)
{
.local .align 16 .b8 __local_depot0[48];
.reg .b64 %SP;
.reg .b64 %SPL;
.reg .pred %p<4>;
.reg .b32 %r<140>;
.reg .b64 %rd<39>;
//
.shared .align 2 .b8 _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_A[512];
//
.shared .align 2 .b8 _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_B[256];
//
.shared .align 2 .b8 _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_C[256];

mov.u64 %SPL, __local_depot0;
cvta.local.u64 %SP, %SPL;
ld.param.u64 %rd10, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_0];
ld.param.u64 %rd7, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_1];
ld.param.u64 %rd8, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_2];
ld.param.u64 %rd9, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_4];
ld.param.u64 %rd11, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_5];
add.u64 %rd1, %SPL, 0;
mov.u32 %r1, %tid.x;
and.b32 %r12, %r1, 31;
cvt.u64.u32 %rd2, %r12;
mov.u32 %r13, 4;
mov.u32 %r14, 3;
mov.u32 %r15, 2;
mov.u32 %r16, 1;
st.local.v4.u32 [%rd1], {%r16, %r15, %r14, %r13};
mov.u32 %r17, 8;
mov.u32 %r18, 7;
mov.u32 %r19, 6;
mov.u32 %r20, 5;
st.local.v4.u32 [%rd1+16], {%r20, %r19, %r18, %r17};
shr.u64 %rd13, %rd2, 1;
cvt.u32.u64 %r2, %rd13;
shl.b32 %r21, %r2, 5;
mov.u32 %r22, _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_A;
add.s32 %r23, %r22, %r21;
and.b32 %r3, %r1, 1;
shl.b32 %r24, %r1, 4;
and.b32 %r25, %r24, 16;
add.s32 %r26, %r23, %r25;
mul.lo.s64 %rd4, %rd13, %rd11;
cvta.to.global.u64 %rd14, %rd10;
shl.b64 %rd15, %rd4, 1;
add.s64 %rd16, %rd14, %rd15;
and.b64 %rd5, %rd2, 1;
mul.wide.u32 %rd17, %r1, 16;
and.b64 %rd18, %rd17, 16;
add.s64 %rd19, %rd16, %rd18;
ld.global.nc.v4.u32 {%r27, %r28, %r29, %r30}, [%rd19];
st.shared.v4.u32 [%r26], {%r27, %r28, %r29, %r30};
setp.gt.u32 %p1, %r12, 15;
@%p1 bra $L__BB0_2;

cvta.to.global.u64 %rd20, %rd7;
mov.u32 %r36, _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_B;
add.s32 %r37, %r36, %r21;
shl.b32 %r38, %r3, 4;
add.s32 %r39, %r37, %r38;
add.s64 %rd22, %rd20, %rd15;
shl.b64 %rd23, %rd5, 4;
add.s64 %rd24, %rd22, %rd23;
ld.global.nc.v4.u32 {%r40, %r41, %r42, %r43}, [%rd24];
st.shared.v4.u32 [%r39], {%r40, %r41, %r42, %r43};

$L__BB0_2:
cvt.u32.u64 %r95, %rd2;
bar.sync 0;
shl.b32 %r96, %r1, 5;
and.b32 %r97, %r96, 480;
add.s32 %r99, %r22, %r97;
and.b32 %r100, %r95, 16;
add.s32 %r52, %r99, %r100;
//
ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r48, %r49, %r50, %r51}, [%r52];

//
and.b64 %rd6, %rd2, 7;
cvt.u32.u64 %r101, %rd6;
shl.b32 %r102, %r101, 5;
mov.u32 %r103, _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_B;
add.s32 %r104, %r103, %r102;
shl.b32 %r105, %r1, 1;
and.b32 %r106, %r105, 16;
add.s32 %r55, %r104, %r106;
//
ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r53, %r54}, [%r55];

//
//
mma.sync.aligned.m8n8k4.row.col.f16.f16.f16.f16 {%r56, %r57, %r58, %r59}, {%r48, %r49}, {%r53, %r54}, {%r20, %r19, %r18, %r17};

//
st.local.v2.u32 [%rd1+8], {%r58, %r59};
//
mma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f32 {%r68, %r69, %r70, %r71, %r72, %r73, %r74, %r75}, {%r50, %r51}, {%r107, %r108}, {%r56, %r57, %r58, %r59, %r20, %r19, %r18, %r17};

//
st.local.v4.u32 [%rd1+16], {%r68, %r69, %r70, %r71};
st.local.v4.u32 [%rd1+32], {%r72, %r73, %r74, %r75};
//
mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r88, %r89}, {%r48, %r49}, {%r53}, {%r56, %r57};

//
st.local.v2.u32 [%rd1], {%r88, %r89};
bar.sync 0;
setp.ne.s32 %p2, %r95, 0;
@%p2 bra $L__BB0_4;

add.u64 %rd25, %SP, 32;
add.u64 %rd26, %SPL, 32;
mov.u64 %rd27, $str;
cvta.global.u64 %rd28, %rd27;
{ //
.reg .b32 temp_param_reg;
.param .b64 param0;
st.param.b64 [param0+0], %rd28;
.param .b64 param1;
st.param.b64 [param1+0], 0;
.param .b32 retval0;
call.uni (retval0), 
vprintf, 
(
param0, 
param1
);
ld.param.b32 %r109, [retval0+0];
} //
st.local.u32 [%rd26], %r88;
mov.u64 %rd29, $str$1;
cvta.global.u64 %rd30, %rd29;
{ //
.reg .b32 temp_param_reg;
.param .b64 param0;
st.param.b64 [param0+0], %rd30;
.param .b64 param1;
st.param.b64 [param1+0], %rd25;
.param .b32 retval0;
call.uni (retval0), 
vprintf, 
(
param0, 
param1
);
ld.param.b32 %r110, [retval0+0];
} //
st.local.u32 [%rd26], %r89;
{ //
.reg .b32 temp_param_reg;
.param .b64 param0;
st.param.b64 [param0+0], %rd30;
.param .b64 param1;
st.param.b64 [param1+0], %rd25;
.param .b32 retval0;
call.uni (retval0), 
vprintf, 
(
param0, 
param1
);
ld.param.b32 %r111, [retval0+0];
} //
st.local.u32 [%rd26], %r58;
{ //
.reg .b32 temp_param_reg;
.param .b64 param0;
st.param.b64 [param0+0], %rd30;
.param .b64 param1;
st.param.b64 [param1+0], %rd25;
.param .b32 retval0;
call.uni (retval0), 
vprintf, 
(
param0, 
param1
);
ld.param.b32 %r112, [retval0+0];
} //
st.local.u32 [%rd26], %r59;
{ //
.reg .b32 temp_param_reg;
.param .b64 param0;
st.param.b64 [param0+0], %rd30;
.param .b64 param1;
st.param.b64 [param1+0], %rd25;
.param .b32 retval0;
call.uni (retval0), 
vprintf, 
(
param0, 
param1
);
ld.param.b32 %r113, [retval0+0];
} //
st.local.u32 [%rd26], %r68;
{ //
.reg .b32 temp_param_reg;
.param .b64 param0;
st.param.b64 [param0+0], %rd30;
.param .b64 param1;
st.param.b64 [param1+0], %rd25;
.param .b32 retval0;
call.uni (retval0), 
vprintf, 
(
param0, 
param1
);
ld.param.b32 %r114, [retval0+0];
} //
st.local.u32 [%rd26], %r69;
{ //
.reg .b32 temp_param_reg;
.param .b64 param0;
st.param.b64 [param0+0], %rd30;
.param .b64 param1;
st.param.b64 [param1+0], %rd25;
.param .b32 retval0;
call.uni (retval0), 
vprintf, 
(
param0, 
param1
);
ld.param.b32 %r115, [retval0+0];
} //
st.local.u32 [%rd26], %r70;
{ //
.reg .b32 temp_param_reg;
.param .b64 param0;
st.param.b64 [param0+0], %rd30;
.param .b64 param1;
st.param.b64 [param1+0], %rd25;
.param .b32 retval0;
call.uni (retval0), 
vprintf, 
(
param0, 
param1
);
ld.param.b32 %r116, [retval0+0];
} //
st.local.u32 [%rd26], %r71;
{ //
.reg .b32 temp_param_reg;
.param .b64 param0;
st.param.b64 [param0+0], %rd30;
.param .b64 param1;
st.param.b64 [param1+0], %rd25;
.param .b32 retval0;
call.uni (retval0), 
vprintf, 
(
param0, 
param1
);
ld.param.b32 %r117, [retval0+0];
} //
mov.u64 %rd31, $str$2;
cvta.global.u64 %rd32, %rd31;
{ //
.reg .b32 temp_param_reg;
.param .b64 param0;
st.param.b64 [param0+0], %rd32;
.param .b64 param1;
st.param.b64 [param1+0], 0;
.param .b32 retval0;
call.uni (retval0), 
vprintf, 
(
param0, 
param1
);
ld.param.b32 %r118, [retval0+0];
} //

$L__BB0_4:
setp.gt.u32 %p3, %r95, 15;
shl.b64 %rd33, %rd6, 2;
add.s64 %rd34, %rd1, %rd33;
ld.local.u32 %r120, [%rd34];
shl.b32 %r121, %r95, 2;
and.b32 %r122, %r121, 112;
mov.u32 %r123, _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_C;
add.s32 %r124, %r123, %r122;
shl.b32 %r125, %r1, 2;
and.b32 %r126, %r125, 12;
add.s32 %r127, %r124, %r126;
st.shared.u32 [%r127], %r120;
st.shared.u32 [%r127+128], %r89;
bar.sync 0;
@%p3 bra $L__BB0_6;

mul.lo.s64 %rd35, %rd2, %rd9;
cvta.to.global.u64 %rd36, %rd8;
shl.b64 %rd37, %rd35, 1;
add.s64 %rd38, %rd36, %rd37;
shl.b32 %r129, %r95, 4;
add.s32 %r131, %r123, %r129;
ld.shared.v4.u32 {%r132, %r133, %r134, %r135}, [%r131];
st.global.v4.u32 [%rd38], {%r132, %r133, %r134, %r135};

$L__BB0_6:
bar.sync 0;
ret;

}


