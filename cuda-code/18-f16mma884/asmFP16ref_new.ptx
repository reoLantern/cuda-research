
Fatbin elf code:
================
arch = sm_86
code version = [1,7]
host = linux
compile_size = 64bit

Fatbin elf code:
================
arch = sm_86
code version = [1,7]
host = linux
compile_size = 64bit

Fatbin ptx code:
================
arch = sm_86
code version = [8,4]
host = linux
compile_size = 64bit
compressed
ptxasOptions = 

//
//
//
//
//
//

.version 8.4
.target sm_86
.address_size 64

//
//
//
//

.visible .entry _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm(
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_0,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_1,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_2,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_3,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_4,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_5
)
{
.reg .pred %p<3>;
.reg .b32 %r<89>;
.reg .b64 %rd<25>;
//
.shared .align 2 .b8 _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_A[512];
//
.shared .align 2 .b8 _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_B[256];
//
.shared .align 2 .b8 _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_C[256];

ld.param.u64 %rd7, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_0];
ld.param.u64 %rd4, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_1];
ld.param.u64 %rd5, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_2];
ld.param.u64 %rd6, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_4];
ld.param.u64 %rd8, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_5];
cvta.to.global.u64 %rd9, %rd7;
mov.u32 %r2, %tid.x;
and.b32 %r3, %r2, 31;
cvt.u64.u32 %rd1, %r3;
shr.u64 %rd10, %rd1, 1;
cvt.u32.u64 %r1, %rd10;
shl.b32 %r4, %r1, 5;
mov.u32 %r5, _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_A;
add.s32 %r6, %r5, %r4;
shl.b32 %r7, %r2, 4;
and.b32 %r8, %r7, 16;
add.s32 %r9, %r6, %r8;
mul.lo.s64 %rd2, %rd10, %rd8;
shl.b64 %rd11, %rd2, 1;
add.s64 %rd12, %rd9, %rd11;
and.b64 %rd3, %rd1, 1;
mul.wide.u32 %rd13, %r2, 16;
and.b64 %rd14, %rd13, 16;
add.s64 %rd15, %rd12, %rd14;
ld.global.nc.v4.u32 {%r10, %r11, %r12, %r13}, [%rd15];
st.shared.v4.u32 [%r9], {%r10, %r11, %r12, %r13};
setp.gt.u32 %p1, %r3, 15;
@%p1 bra $L__BB0_2;

cvta.to.global.u64 %rd16, %rd4;
mov.u32 %r19, _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_B;
add.s32 %r20, %r19, %r4;
add.s32 %r24, %r20, %r8;
add.s64 %rd18, %rd16, %rd11;
shl.b64 %rd19, %rd3, 4;
add.s64 %rd20, %rd18, %rd19;
ld.global.nc.v4.u32 {%r25, %r26, %r27, %r28}, [%rd20];
st.shared.v4.u32 [%r24], {%r25, %r26, %r27, %r28};

$L__BB0_2:
cvt.u32.u64 %r58, %rd1;
setp.gt.u32 %p2, %r58, 15;
bar.sync 0;
shl.b32 %r60, %r2, 5;
and.b32 %r61, %r60, 480;
add.s32 %r63, %r5, %r61;
and.b32 %r64, %r58, 16;
add.s32 %r37, %r63, %r64;
//
ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r33, %r34, %r35, %r36}, [%r37];

//
and.b32 %r65, %r60, 224;
mov.u32 %r66, _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_B;
add.s32 %r67, %r66, %r65;
shl.b32 %r68, %r2, 1;
and.b32 %r69, %r68, 16;
add.s32 %r40, %r67, %r69;
//
ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r38, %r39}, [%r40];

//
mov.u32 %r50, 0;
//
mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r41, %r42}, {%r33, %r34, %r35, %r36}, {%r38, %r39}, {%r50, %r50};

//
//
mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r51, %r52}, {%r33, %r34}, {%r38}, {%r41, %r42};

//
bar.sync 0;
shl.b32 %r70, %r58, 2;
and.b32 %r71, %r70, 112;
mov.u32 %r72, _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_C;
add.s32 %r73, %r72, %r71;
shl.b32 %r74, %r2, 2;
and.b32 %r75, %r74, 12;
add.s32 %r76, %r73, %r75;
st.shared.u32 [%r76], %r51;
st.shared.u32 [%r76+128], %r52;
bar.sync 0;
@%p2 bra $L__BB0_4;

cvta.to.global.u64 %rd21, %rd5;
mul.lo.s64 %rd22, %rd1, %rd6;
shl.b64 %rd23, %rd22, 1;
add.s64 %rd24, %rd21, %rd23;
shl.b32 %r78, %r58, 4;
add.s32 %r80, %r72, %r78;
ld.shared.v4.u32 {%r81, %r82, %r83, %r84}, [%r80];
st.global.v4.u32 [%rd24], {%r81, %r82, %r83, %r84};

$L__BB0_4:
bar.sync 0;
ret;

}


