
Fatbin elf code:
================
arch = sm_86
code version = [1,7]
host = linux
compile_size = 64bit

Fatbin elf code:
================
arch = sm_86
code version = [1,7]
host = linux
compile_size = 64bit

Fatbin ptx code:
================
arch = sm_86
code version = [8,4]
host = linux
compile_size = 64bit
compressed
ptxasOptions = 

//
//
//
//
//
//

.version 8.4
.target sm_86
.address_size 64

//
//
//
//

.visible .entry _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm(
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_0,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_1,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_2,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_3,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_4,
.param .u64 _Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_5
)
{
.local .align 8 .b8 __local_depot0[40];
.reg .b64 %SP;
.reg .b64 %SPL;
.reg .pred %p<3>;
.reg .b32 %r<136>;
.reg .b64 %rd<33>;
//
.shared .align 2 .b8 _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_A[512];
//
.shared .align 2 .b8 _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_B[256];
//
.shared .align 2 .b8 _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_C[256];

mov.u64 %SPL, __local_depot0;
ld.param.u64 %rd9, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_0];
ld.param.u64 %rd7, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_1];
ld.param.u64 %rd10, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_2];
ld.param.u64 %rd8, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_4];
ld.param.u64 %rd11, [_Z19mma16816NaiveKernelPK6__halfS1_PS_mmm_param_5];
cvta.to.global.u64 %rd1, %rd10;
add.u64 %rd2, %SPL, 0;
mov.u32 %r1, %tid.x;
and.b32 %r9, %r1, 31;
shl.b32 %r10, %r1, 5;
and.b32 %r11, %r10, 992;
mul.wide.u32 %rd13, %r11, 2;
add.s64 %rd14, %rd1, %rd13;
ld.global.u32 %r2, [%rd14+2];
ld.global.u32 %r12, [%rd14];
st.local.v2.u32 [%rd2], {%r12, %r2};
ld.global.u32 %r3, [%rd14+4];
ld.global.u32 %r4, [%rd14+6];
st.local.v2.u32 [%rd2+8], {%r3, %r4};
ld.global.u32 %r13, [%rd14+10];
ld.global.u32 %r14, [%rd14+8];
st.local.v2.u32 [%rd2+16], {%r14, %r13};
ld.global.u32 %r15, [%rd14+14];
ld.global.u32 %r16, [%rd14+12];
st.local.v2.u32 [%rd2+24], {%r16, %r15};
ld.global.u32 %r5, [%rd14+16];
ld.global.u32 %r6, [%rd14+18];
st.local.v2.u32 [%rd2+32], {%r5, %r6};
cvt.u64.u32 %rd4, %r9;
shr.u64 %rd15, %rd4, 1;
cvt.u32.u64 %r7, %rd15;
shl.b32 %r17, %r7, 5;
mov.u32 %r18, _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_A;
add.s32 %r19, %r18, %r17;
and.b32 %r8, %r1, 1;
shl.b32 %r20, %r1, 4;
and.b32 %r21, %r20, 16;
add.s32 %r22, %r19, %r21;
mul.lo.s64 %rd5, %rd15, %rd11;
cvta.to.global.u64 %rd16, %rd9;
shl.b64 %rd17, %rd5, 1;
add.s64 %rd18, %rd16, %rd17;
and.b64 %rd6, %rd4, 1;
mul.wide.u32 %rd19, %r1, 16;
and.b64 %rd20, %rd19, 16;
add.s64 %rd21, %rd18, %rd20;
ld.global.nc.v4.u32 {%r23, %r24, %r25, %r26}, [%rd21];
st.shared.v4.u32 [%r22], {%r23, %r24, %r25, %r26};
setp.gt.u32 %p1, %r9, 15;
@%p1 bra $L__BB0_2;

cvta.to.global.u64 %rd22, %rd7;
mov.u32 %r32, _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_B;
add.s32 %r33, %r32, %r17;
shl.b32 %r34, %r8, 4;
add.s32 %r35, %r33, %r34;
add.s64 %rd24, %rd22, %rd17;
shl.b64 %rd25, %rd6, 4;
add.s64 %rd26, %rd24, %rd25;
ld.global.nc.v4.u32 {%r36, %r37, %r38, %r39}, [%rd26];
st.shared.v4.u32 [%r35], {%r36, %r37, %r38, %r39};

$L__BB0_2:
cvt.u32.u64 %r97, %rd4;
setp.gt.u32 %p2, %r97, 15;
bar.sync 0;
and.b32 %r99, %r10, 480;
add.s32 %r101, %r18, %r99;
and.b32 %r102, %r97, 16;
add.s32 %r48, %r101, %r102;
//
ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r44, %r45, %r46, %r47}, [%r48];

//
shl.b32 %r103, %r97, 5;
and.b32 %r104, %r103, 224;
mov.u32 %r105, _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_B;
add.s32 %r106, %r105, %r104;
shl.b32 %r107, %r1, 1;
and.b32 %r108, %r107, 16;
add.s32 %r51, %r106, %r108;
//
ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r49, %r50}, [%r51];

//
//
mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r52, %r53}, {%r44, %r45}, {%r49}, {%r3, %r2};

//
//
mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r59, %r60}, {%r46, %r47}, {%r109}, {%r52, %r53};

//
st.local.u32 [%rd2+20], %r60;
add.s32 %r74, %r52, %r59;
//
mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32 {%r66, %r67, %r68, %r69}, {%r46, %r47}, {%r110}, {%r52, %r74, %r3, %r4};

//
st.local.u32 [%rd2+8], %r67;
bar.sync 0;
//
mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r77, %r78}, {%r44, %r45, %r46, %r47}, {%r111, %r112}, {%r5, %r6};

//
st.local.u32 [%rd2+28], %r77;
st.local.u32 [%rd2+32], %r78;
//
mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r87, %r88}, {%r44, %r45, %r46, %r47}, {%r50, %r113}, {%r77, %r78};

//
st.local.u32 [%rd2+12], %r87;
st.local.u32 [%rd2+16], %r88;
add.s32 %r114, %r60, %r66;
add.s32 %r115, %r52, %r88;
st.local.v2.u32 [%rd2], {%r115, %r114};
shl.b64 %rd27, %rd4, 2;
and.b64 %rd28, %rd27, 28;
add.s64 %rd29, %rd2, %rd28;
ld.local.u32 %r116, [%rd29];
shl.b32 %r117, %r97, 2;
and.b32 %r118, %r117, 112;
mov.u32 %r119, _ZZ19mma16816NaiveKernelPK6__halfS1_PS_mmmE7shmem_C;
add.s32 %r120, %r119, %r118;
shl.b32 %r121, %r1, 2;
and.b32 %r122, %r121, 12;
add.s32 %r123, %r120, %r122;
st.shared.u32 [%r123], %r116;
st.shared.u32 [%r123+128], %r114;
bar.sync 0;
@%p2 bra $L__BB0_4;

mul.lo.s64 %rd30, %rd4, %rd8;
shl.b64 %rd31, %rd30, 1;
add.s64 %rd32, %rd1, %rd31;
shl.b32 %r125, %r97, 4;
add.s32 %r127, %r119, %r125;
ld.shared.v4.u32 {%r128, %r129, %r130, %r131}, [%r127];
st.global.v4.u32 [%rd32], {%r128, %r129, %r130, %r131};

$L__BB0_4:
bar.sync 0;
ret;

}


