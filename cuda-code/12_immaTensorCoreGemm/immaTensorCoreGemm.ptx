
Fatbin elf code:
================
arch = sm_86
code version = [1,7]
producer = <unknown>
host = linux
compile_size = 64bit

Fatbin elf code:
================
arch = sm_86
code version = [1,7]
producer = <unknown>
host = linux
compile_size = 64bit

Fatbin ptx code:
================
arch = sm_86
code version = [7,3]
producer = <unknown>
host = linux
compile_size = 64bit
compressed








.version 7.3
.target sm_86
.address_size 64



.visible .entry _Z17compute_gemm_immaPKhS0_PKiPiii(
.param .u64 _Z17compute_gemm_immaPKhS0_PKiPiii_param_0,
.param .u64 _Z17compute_gemm_immaPKhS0_PKiPiii_param_1,
.param .u64 _Z17compute_gemm_immaPKhS0_PKiPiii_param_2,
.param .u64 _Z17compute_gemm_immaPKhS0_PKiPiii_param_3,
.param .u32 _Z17compute_gemm_immaPKhS0_PKiPiii_param_4,
.param .u32 _Z17compute_gemm_immaPKhS0_PKiPiii_param_5
)
{



ret;

}

.visible .entry _Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii(
.param .u64 _Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_0,
.param .u64 _Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_1,
.param .u64 _Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_2,
.param .u64 _Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_3,
.param .u32 _Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_4,
.param .u32 _Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_5,
.param .u32 _Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_6
)
{
.reg .pred %p<8>;
.reg .b32 %r<57>;
.reg .b64 %rd<17>;


ld.param.u64 %rd3, [_Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_0];
ld.param.u64 %rd4, [_Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_1];
ld.param.u64 %rd5, [_Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_2];
ld.param.u64 %rd6, [_Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_3];
ld.param.u32 %r24, [_Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_4];
ld.param.u32 %r21, [_Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_5];
ld.param.u32 %r22, [_Z25simple_wmma_gemm_imma8832PKjS0_PKiPiiii_param_6];
mov.u32 %r25, %ntid.x;
mov.u32 %r26, %ctaid.x;
mov.u32 %r27, %tid.x;
mad.lo.s32 %r28, %r26, %r25, %r27;
mov.u32 %r29, WARP_SZ;
div.u32 %r30, %r28, %r29;
mov.u32 %r31, %ntid.y;
mov.u32 %r32, %ctaid.y;
mov.u32 %r33, %tid.y;
mad.lo.s32 %r34, %r32, %r31, %r33;
shl.b32 %r1, %r34, 3;
shl.b32 %r2, %r30, 3;
setp.lt.s32 %p2, %r2, %r24;
setp.lt.s32 %p3, %r1, %r21;
and.pred %p1, %p3, %p2;
not.pred %p4, %p1;
@%p4 bra $L__BB1_2;

cvta.to.global.u64 %rd7, %rd5;
mad.lo.s32 %r35, %r2, %r21, %r1;
mul.wide.s32 %rd8, %r35, 4;
add.s64 %rd9, %rd7, %rd8;
wmma.load.c.sync.aligned.row.m8n8k32.global.s32 {%r54, %r53}, [%rd9], %r21;

$L__BB1_2:
setp.lt.s32 %p5, %r22, 1;
@%p5 bra $L__BB1_7;

cvta.to.global.u64 %rd1, %rd4;
cvta.to.global.u64 %rd2, %rd3;
shr.s32 %r38, %r22, 31;
shr.u32 %r39, %r38, 29;
add.s32 %r40, %r22, %r39;
shr.s32 %r41, %r40, 3;
mul.lo.s32 %r7, %r2, %r41;
mul.lo.s32 %r8, %r41, %r1;
mov.u32 %r49, 0;
mov.u32 %r50, %r49;

$L__BB1_4:
@%p4 bra $L__BB1_6;

add.s32 %r42, %r49, %r7;
mul.wide.s32 %rd10, %r42, 4;
add.s64 %rd11, %rd2, %rd10;
wmma.load.a.sync.aligned.row.m8n8k32.global.s4 {%r43}, [%rd11], %r22;
add.s32 %r44, %r49, %r8;
mul.wide.s32 %rd12, %r44, 4;
add.s64 %rd13, %rd1, %rd12;
wmma.load.b.sync.aligned.col.m8n8k32.global.s4 {%r45}, [%rd13], %r22;
wmma.mma.sync.aligned.row.col.m8n8k32.s32.s4.s4.s32 {%r54, %r53}, {%r43}, {%r45}, {%r54, %r53};

$L__BB1_6:
add.s32 %r49, %r49, 4;
add.s32 %r50, %r50, 32;
setp.lt.s32 %p7, %r50, %r22;
@%p7 bra $L__BB1_4;

$L__BB1_7:
cvta.to.global.u64 %rd14, %rd6;
mad.lo.s32 %r46, %r2, %r21, %r1;
mul.wide.s32 %rd15, %r46, 4;
add.s64 %rd16, %rd14, %rd15;
wmma.store.d.sync.aligned.row.m8n8k32.global.s32 [%rd16], {%r54, %r53}, %r21;
ret;

}

.visible .entry _Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii(
.param .u64 _Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_0,
.param .u64 _Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_1,
.param .u64 _Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_2,
.param .u64 _Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_3,
.param .u32 _Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_4,
.param .u32 _Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_5,
.param .u32 _Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_6
)
{
.reg .pred %p<8>;
.reg .b32 %r<69>;
.reg .b64 %rd<41>;


ld.param.u64 %rd5, [_Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_0];
ld.param.u64 %rd6, [_Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_1];
ld.param.u64 %rd7, [_Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_2];
ld.param.u64 %rd8, [_Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_3];
ld.param.u32 %r26, [_Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_4];
ld.param.u32 %r23, [_Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_5];
ld.param.u32 %r24, [_Z24simple_mma_gemm_imma8832PKjS0_PKiPiiii_param_6];
mov.u32 %r27, %ntid.x;
mov.u32 %r28, %ctaid.x;
mov.u32 %r1, %tid.x;
mad.lo.s32 %r29, %r28, %r27, %r1;
mov.u32 %r30, WARP_SZ;
div.u32 %r31, %r29, %r30;
mov.u32 %r32, %ntid.y;
mov.u32 %r33, %ctaid.y;
mov.u32 %r34, %tid.y;
mad.lo.s32 %r35, %r33, %r32, %r34;
and.b32 %r2, %r1, 31;
shl.b32 %r3, %r35, 3;
shl.b32 %r4, %r31, 3;
setp.lt.s32 %p2, %r4, %r26;
setp.lt.s32 %p3, %r3, %r23;
and.pred %p1, %p3, %p2;
not.pred %p4, %p1;
@%p4 bra $L__BB2_2;

cvta.to.global.u64 %rd9, %rd7;
mul.lo.s32 %r36, %r4, %r23;
cvt.s64.s32 %rd10, %r36;
shr.u32 %r37, %r2, 2;
mul.lo.s32 %r38, %r37, %r23;
cvt.u64.u32 %rd11, %r38;
shl.b32 %r39, %r1, 1;
and.b32 %r40, %r39, 6;
cvt.u64.u32 %rd12, %r40;
cvt.s64.s32 %rd13, %r3;
add.s64 %rd14, %rd13, %rd12;
add.s64 %rd15, %rd14, %rd11;
add.s64 %rd16, %rd15, %rd10;
shl.b64 %rd17, %rd16, 2;
add.s64 %rd18, %rd9, %rd17;
ld.global.u32 %r66, [%rd18];
ld.global.u32 %r65, [%rd18+4];

$L__BB2_2:
shr.u32 %r9, %r2, 2;
and.b32 %r10, %r1, 3;
setp.lt.s32 %p5, %r24, 1;
@%p5 bra $L__BB2_7;

shr.s32 %r43, %r24, 31;
shr.u32 %r44, %r43, 29;
add.s32 %r45, %r24, %r44;
shr.s32 %r46, %r45, 3;
mul.lo.s32 %r47, %r4, %r46;
cvt.s64.s32 %rd19, %r47;
mul.lo.s32 %r48, %r46, %r9;
cvt.u64.u32 %rd20, %r48;
cvt.u64.u32 %rd21, %r10;
add.s64 %rd22, %rd20, %rd21;
add.s64 %rd1, %rd22, %rd19;
mul.lo.s32 %r49, %r46, %r3;
cvt.s64.s32 %rd23, %r49;
add.s64 %rd2, %rd22, %rd23;
cvta.to.global.u64 %rd3, %rd6;
cvta.to.global.u64 %rd4, %rd5;
mov.u32 %r61, 0;
mov.u32 %r62, %r61;

$L__BB2_4:
@%p4 bra $L__BB2_6;

cvt.u64.u32 %rd24, %r61;
add.s64 %rd25, %rd1, %rd24;
shl.b64 %rd26, %rd25, 2;
add.s64 %rd27, %rd4, %rd26;
ld.global.u32 %r52, [%rd27];
add.s64 %rd28, %rd2, %rd24;
shl.b64 %rd29, %rd28, 2;
add.s64 %rd30, %rd3, %rd29;
ld.global.u32 %r53, [%rd30];

	mma.sync.aligned.m8n8k32.row.col.s32.s4.s4.s32 {%r66, %r65}, {%r52}, {%r53}, {%r66, %r65};



$L__BB2_6:
add.s32 %r61, %r61, 4;
add.s32 %r62, %r62, 32;
setp.lt.s32 %p7, %r62, %r24;
@%p7 bra $L__BB2_4;

$L__BB2_7:
mul.lo.s32 %r56, %r4, %r23;
cvt.s64.s32 %rd31, %r56;
mul.lo.s32 %r57, %r9, %r23;
cvt.u64.u32 %rd32, %r57;
shl.b32 %r58, %r10, 1;
cvt.u64.u32 %rd33, %r58;
cvt.s64.s32 %rd34, %r3;
add.s64 %rd35, %rd34, %rd33;
add.s64 %rd36, %rd35, %rd32;
add.s64 %rd37, %rd36, %rd31;
cvta.to.global.u64 %rd38, %rd8;
shl.b64 %rd39, %rd37, 2;
add.s64 %rd40, %rd38, %rd39;
st.global.u32 [%rd40], %r66;
st.global.u32 [%rd40+4], %r65;
ret;

}

.visible .entry _Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii(
.param .u64 _Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_0,
.param .u64 _Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_1,
.param .u64 _Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_2,
.param .u64 _Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_3,
.param .u32 _Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_4,
.param .u32 _Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_5,
.param .u32 _Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_6
)
{
.reg .pred %p<8>;
.reg .b32 %r<96>;
.reg .b64 %rd<47>;


ld.param.u64 %rd5, [_Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_0];
ld.param.u64 %rd6, [_Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_1];
ld.param.u64 %rd7, [_Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_2];
ld.param.u64 %rd8, [_Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_3];
ld.param.u32 %r37, [_Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_4];
ld.param.u32 %r34, [_Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_5];
ld.param.u32 %r35, [_Z25simple_mma_gemm_imma16832PKjS0_PKiPiiii_param_6];
mov.u32 %r38, %ntid.x;
mov.u32 %r39, %ctaid.x;
mov.u32 %r1, %tid.x;
mad.lo.s32 %r40, %r39, %r38, %r1;
mov.u32 %r41, WARP_SZ;
div.u32 %r42, %r40, %r41;
mov.u32 %r43, %ntid.y;
mov.u32 %r44, %ctaid.y;
mov.u32 %r45, %tid.y;
mad.lo.s32 %r46, %r44, %r43, %r45;
and.b32 %r2, %r1, 31;
shl.b32 %r3, %r46, 3;
shl.b32 %r4, %r42, 4;
setp.lt.s32 %p2, %r4, %r37;
setp.lt.s32 %p3, %r3, %r34;
and.pred %p1, %p3, %p2;
not.pred %p4, %p1;
@%p4 bra $L__BB3_2;

cvt.s64.s32 %rd9, %r3;
mul.lo.s32 %r47, %r4, %r34;
cvt.s64.s32 %rd10, %r47;
shr.u32 %r48, %r2, 2;
mul.lo.s32 %r49, %r48, %r34;
cvt.u64.u32 %rd11, %r49;
shl.b32 %r50, %r1, 1;
and.b32 %r51, %r50, 6;
cvt.u64.u32 %rd12, %r51;
add.s64 %rd13, %rd9, %rd12;
add.s64 %rd14, %rd13, %rd11;
add.s64 %rd15, %rd14, %rd10;
cvta.to.global.u64 %rd16, %rd7;
shl.b64 %rd17, %rd15, 2;
add.s64 %rd18, %rd16, %rd17;
ld.global.u32 %r91, [%rd18];
ld.global.u32 %r90, [%rd18+4];
shl.b32 %r52, %r34, 3;
mul.wide.s32 %rd19, %r52, 4;
add.s64 %rd20, %rd18, %rd19;
ld.global.u32 %r88, [%rd20];

$L__BB3_2:
shr.u32 %r11, %r2, 2;
and.b32 %r12, %r1, 3;
setp.lt.s32 %p5, %r35, 1;
@%p5 bra $L__BB3_7;

shr.s32 %r57, %r35, 31;
shr.u32 %r58, %r57, 29;
add.s32 %r59, %r35, %r58;
shr.s32 %r60, %r59, 3;
mul.lo.s32 %r61, %r4, %r60;
cvt.s64.s32 %rd21, %r61;
mul.lo.s32 %r62, %r60, %r11;
cvt.u64.u32 %rd22, %r62;
cvt.u64.u32 %rd23, %r12;
add.s64 %rd24, %rd22, %rd23;
add.s64 %rd1, %rd24, %rd21;
and.b32 %r13, %r59, -8;
mul.lo.s32 %r63, %r60, %r3;
cvt.s64.s32 %rd25, %r63;
add.s64 %rd2, %rd24, %rd25;
cvta.to.global.u64 %rd3, %rd6;
cvta.to.global.u64 %rd4, %rd5;
mov.u32 %r82, 0;
mov.u32 %r83, %r82;

$L__BB3_4:
@%p4 bra $L__BB3_6;

cvt.u64.u32 %rd26, %r82;
add.s64 %rd27, %rd1, %rd26;
shl.b64 %rd28, %rd27, 2;
add.s64 %rd29, %rd4, %rd28;
ld.global.u32 %r68, [%rd29];
mul.wide.s32 %rd30, %r13, 4;
add.s64 %rd31, %rd29, %rd30;
ld.global.u32 %r69, [%rd31];
add.s64 %rd32, %rd2, %rd26;
shl.b64 %rd33, %rd32, 2;
add.s64 %rd34, %rd3, %rd33;
ld.global.u32 %r70, [%rd34];

	mma.sync.aligned.m16n8k32.row.col.s32.s4.s4.s32 {%r91, %r90, %r89, %r88}, {%r68, %r69}, {%r70}, {%r91, %r90, %r89, %r88};



$L__BB3_6:
add.s32 %r82, %r82, 4;
add.s32 %r83, %r83, 32;
setp.lt.s32 %p7, %r83, %r35;
@%p7 bra $L__BB3_4;

$L__BB3_7:
mul.lo.s32 %r75, %r4, %r34;
cvt.s64.s32 %rd35, %r75;
mul.lo.s32 %r76, %r11, %r34;
cvt.u64.u32 %rd36, %r76;
shl.b32 %r77, %r12, 1;
cvt.u64.u32 %rd37, %r77;
cvt.s64.s32 %rd38, %r3;
add.s64 %rd39, %rd38, %rd37;
add.s64 %rd40, %rd39, %rd36;
add.s64 %rd41, %rd40, %rd35;
cvta.to.global.u64 %rd42, %rd8;
shl.b64 %rd43, %rd41, 2;
add.s64 %rd44, %rd42, %rd43;
st.global.u32 [%rd44], %r91;
st.global.u32 [%rd44+4], %r90;
shl.b32 %r78, %r34, 3;
mul.wide.s32 %rd45, %r78, 4;
add.s64 %rd46, %rd44, %rd45;
st.global.u32 [%rd46], %r89;
st.global.u32 [%rd46+4], %r88;
ret;

}

